{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor([4.], grad_fn=<MulBackward0>)\n",
      "grad:  1.0 2.0 tensor(-2.)\n",
      "grad:  2.0 4.0 tensor(-7.8400)\n",
      "grad:  3.0 6.0 tensor(-16.2288)\n",
      "progress: 0 tensor(7.3159)\n",
      "grad:  1.0 2.0 tensor(-1.4786)\n",
      "grad:  2.0 4.0 tensor(-5.7962)\n",
      "grad:  3.0 6.0 tensor(-11.9981)\n",
      "progress: 1 tensor(3.9988)\n",
      "grad:  1.0 2.0 tensor(-1.0932)\n",
      "grad:  2.0 4.0 tensor(-4.2852)\n",
      "grad:  3.0 6.0 tensor(-8.8704)\n",
      "progress: 2 tensor(2.1857)\n",
      "grad:  1.0 2.0 tensor(-0.8082)\n",
      "grad:  2.0 4.0 tensor(-3.1681)\n",
      "grad:  3.0 6.0 tensor(-6.5580)\n",
      "progress: 3 tensor(1.1946)\n",
      "grad:  1.0 2.0 tensor(-0.5975)\n",
      "grad:  2.0 4.0 tensor(-2.3422)\n",
      "grad:  3.0 6.0 tensor(-4.8484)\n",
      "progress: 4 tensor(0.6530)\n",
      "grad:  1.0 2.0 tensor(-0.4417)\n",
      "grad:  2.0 4.0 tensor(-1.7316)\n",
      "grad:  3.0 6.0 tensor(-3.5845)\n",
      "progress: 5 tensor(0.3569)\n",
      "grad:  1.0 2.0 tensor(-0.3266)\n",
      "grad:  2.0 4.0 tensor(-1.2802)\n",
      "grad:  3.0 6.0 tensor(-2.6500)\n",
      "progress: 6 tensor(0.1951)\n",
      "grad:  1.0 2.0 tensor(-0.2414)\n",
      "grad:  2.0 4.0 tensor(-0.9465)\n",
      "grad:  3.0 6.0 tensor(-1.9592)\n",
      "progress: 7 tensor(0.1066)\n",
      "grad:  1.0 2.0 tensor(-0.1785)\n",
      "grad:  2.0 4.0 tensor(-0.6997)\n",
      "grad:  3.0 6.0 tensor(-1.4485)\n",
      "progress: 8 tensor(0.0583)\n",
      "grad:  1.0 2.0 tensor(-0.1320)\n",
      "grad:  2.0 4.0 tensor(-0.5173)\n",
      "grad:  3.0 6.0 tensor(-1.0709)\n",
      "progress: 9 tensor(0.0319)\n",
      "predict (after training) 4 tensor([7.8049], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]\n",
    " \n",
    "w=torch.tensor([1.0],requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    return x*w\n",
    "def loss(x,y):\n",
    "    y_pred=forward(x)\n",
    "    return (y_pred-y)*(y_pred-y)\n",
    "\n",
    "# Training loop\n",
    "print(\"predict (before training)\" , 4, forward(4))\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data): #1\n",
    "        l = loss(x_val, y_val) #loss 계산\n",
    "        l.backward() #여기서 부터 back propagation\n",
    "        print(\"grad: \", x_val, y_val,w.grad.data[0])# l을 w로 편미분 한 값, 0 으로 초기화 후 업데이트\n",
    "        w.data = w.data - 0.01 * w.grad.data #w 업데이트\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_() #5\n",
    " \n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "print(\"predict (after training)\" , 4, forward(4))# forward(4): 4를 input으로 넣었을 때 정답 label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "x_data = torch.tensor([ [1.0],[2.0],[3.0] ])\n",
    "y_data = torch.tensor([ [2.0],[4.0],[6.0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 4 tensor(7.9898)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(torch.nn.Module):#torch.nn.Module을 상속받음\n",
    "    #class 생성자 같은 느낌임. 처음 만들어질 때 초기화 해줌. 반드시 필요.\n",
    "    def __init__(self):\n",
    "        #부모 class로부터 상속받은 class는 처음 initialize 해줄 때 부모의 init을 해 주어야 한다.\n",
    "        super(MyModel,self).__init__()\n",
    " \n",
    "        self.linear = torch.nn.Linear(1,1) #MyModel에 실질적인 연산을 할 모델을 구성하는 부분.\n",
    "        # 우리는 간단한 모델을 구성할 것이기 때문에\n",
    "        # torch API에서 제공하는 torch.nn.Linear만 가지고 우리의 모델을 구성했다.\n",
    "        # input개수가 한개이고 output 개수도 한개인 Linear model을 만들어주게 된다.\n",
    "         \n",
    "    #forward (예측을 수행)하는 함수. 모델을 만들 때 반드시 필요.\n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "     \n",
    "model = MyModel() #model이라는 변수에 만든 모델을 넣어준다.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "for epoch in range(501):\n",
    "    # 우리는 모든 x 데이터를 매트릭스(행렬) 형태로 모델에게 넘겨준다.\n",
    "    y_pred = model(x_data)\n",
    "     \n",
    "    # criterion이라는 함수를 통해서 예측과 정답을 비교하는 평가를 진행한다.\n",
    "    # 이 때, MSE Loss 를 criterion에 넣었기 때문에 그것을 기준으로 진행하게 된다.\n",
    "    loss = criterion(y_pred, y_data)\n",
    " \n",
    "    #gradient descent 직전에 초기화 해주기.\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    # 구한 loss로부터 back propagation을 통해 각 변수마다 loss에 대한 gradient 를 구해주기\n",
    "    loss.backward()\n",
    "     \n",
    "    # step()이란 함수를 실행시키면 우리가 미리 선언할 때\n",
    "    # 지정해 준 model의 파라미터들이 업데이트 된다.\n",
    "    optimizer.step()\n",
    "    # 이전 글의 기존 for loop을 이용한 방법으로는 데이터를\n",
    "    # 한번에 하나씩 살펴봐야 해서 효율적이지 못했지만 이제는 한번에 묶어서 계산한다.\n",
    "    # 지금 데이터는 3개라 한번에 봐도 문제가 없지만 몇백만개 이상이 되면 문제가 생긴다.\n",
    "    # SGD를 통해서 업데이트를 진행할 경우에는 mini - batch를\n",
    "    # 사용하는 기법을 통해 이를 해결한다. 자세한 방법은 구글링 해 보자.\n",
    "hour_var = torch.tensor([[4.0]])\n",
    "print(\"predict (after training)\",4,model.forward(hour_var).data[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable.data :실제값 접근\n",
    "Variable.grad :기울기\n",
    "Variable.backward():모든 기울기 계산\n",
    "ex)\n",
    "x = Variable(torch.ones(2,2), requires_grad = True)\n",
    "- requires_grad 는 자동으로 업데이트 시킬지 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1hour :  1.0 tensor(0, dtype=torch.uint8)\n",
      "7hour :  7.0 tensor(1, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# F는 activation function 호출 하기위해 사용\n",
    " \n",
    "class MyModel(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "         \n",
    "    def forward(self,x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "x_data = torch.Tensor( [ [1.0],[2.0],[3.0],[4.0] ])\n",
    "y_data = torch.Tensor( [ [0.],[0.],[1.],[1.] ])\n",
    " \n",
    "model = MyModel()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    " \n",
    "for epoch in range(1000):\n",
    "     \n",
    "    y_pred = model(x_data)\n",
    "     \n",
    "    loss = criterion(y_pred,y_data)\n",
    "\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "     \n",
    "hour_var = torch.Tensor([[1.0]])\n",
    "print(\"1hour : \",1.0, model(hour_var).data[0][0]>0.5)\n",
    "hour_var = torch.Tensor([[7.0]])\n",
    "print(\"7hour : \",7.0, model(hour_var).data[0][0]>0.5)\n",
    "#model.forward(hour_var)이 실행된것으로 보임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class DiabetesDataset(Dataset):\n",
    "     \n",
    "    def __init__(self):\n",
    "        xy=np.loadtxt('./data/diabetes.csv.gz',delimiter=',',dtype=np.float32)\n",
    "        #from https://github.com/hunkim/PyTorchZeroToAll\n",
    "        self.len=xy.shape[0]\n",
    "        self.x_data=torch.from_numpy(xy[:,0:-1])\n",
    "        self.y_data=torch.from_numpy(xy[:,[-1]])\n",
    "         \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "     \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset = dataset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=0)\n",
    "class MyModel(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.l1=torch.nn.Linear(8,4)\n",
    "        self.l2=torch.nn.Linear(4,6)\n",
    "        self.l3=torch.nn.Linear(6,1)\n",
    "         \n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "         \n",
    "    def forward(self,x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "model = MyModel()\n",
    " \n",
    "criterion = torch.nn.BCELoss(size_average = True)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    " \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    " \n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data[0])\n",
    " \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad() #gradient 초기화(누적되기 때문)\n",
    "        loss.backward() # gradient 생성, 파라미터 업데이트\n",
    "        optimizer.step() # perform a single optimization step\n",
    "cnt=0\n",
    "for it in range(y_data.size()[0]):\n",
    "    if( (y_pred[it][0]>0.5)==y_data[it][0].type(torch.ByteTensor)):\n",
    "        cnt= cnt+1\n",
    "         \n",
    "print(cnt)\n",
    "print(\"Accuracy : \",cnt*100/y_data.size()[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
